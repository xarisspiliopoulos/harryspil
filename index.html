<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Harry Spiliopoulos</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
</head>
<body class="bg-dark">
<main>
    <div class="container text-center text-secondary">
        <div class="row">
            <div class="col">
                <h3>Harry Spiliopoulos</h3>
                <p class="lead">
                    With a little help from my friends.
                </p>
                <p>csrf token. contains the payload of the action you are about to execute. in the verify screen you get this new token and is valid only for the amount,
                    the action you requested. Then it gets invalidated/not used any more!!!</p>
                <p>JWT : so the claims are not encrypted, anyone can change them, but on the verification process the token will be detected that has been tampered, right?
                    ChatGPT

                    Yes, that's correct. In the context of JWT, the claims in the payload are not encrypted but are base64 URL encoded. This means that anyone who possesses the token can decode the payload and view its contents. However, the integrity of the claims is protected through a digital signature.

                    Here's how it works:

                    Encoding: The JWT consists of three parts: the header, the payload, and the signature.
                    Header: Contains metadata about the type of token and the algorithm used for signing (e.g., HS256).
                    Payload: Contains the claims, which are pieces of information such as user details and permissions.
                    Signature: A hash of the header and payload, created using the secret key and the specified algorithm.

                    Creating the Token:
                    The header and payload are encoded in base64 URL format.
                    The signature is generated by hashing the encoded header and payload together with the secret key.

                    Verification:
                    When a token is received, the header and payload are re-encoded.
                    A new signature is generated using the received header, payload, and the secret key.
                    This new signature is compared to the signature in the token.
                    If the signatures match, the token is considered valid and untampered. If they do not match, the token has been altered and is therefore invalid.

                    Example in Java

                    Here’s a detailed example of creating and verifying a JWT using the java-jwt library.

                    Creating a JWT with Claims:

                    java

                    import com.auth0.jwt.JWT;
                    import com.auth0.jwt.algorithms.Algorithm;
                    import java.util.Base64;
                    import java.security.SecureRandom;

                    public class CreateJWT {
                    public static void main(String[] args) {
                    // Generate a 256-bit secret key
                    SecureRandom secureRandom = new SecureRandom();
                    byte[] key = new byte[32];
                    secureRandom.nextBytes(key);
                    String secret = Base64.getUrlEncoder().withoutPadding().encodeToString(key);

                    Algorithm algorithm = Algorithm.HMAC256(secret);

                    String token = JWT.create()
                    .withClaim("user_id", 123)
                    .withClaim("role", "admin")
                    .sign(algorithm);

                    System.out.println("Generated Token: " + token);
                    System.out.println("Secret Key: " + secret); // Store this securely
                    }
                    }

                    Verifying a JWT:

                    java

                    import com.auth0.jwt.JWT;
                    import com.auth0.jwt.algorithms.Algorithm;
                    import com.auth0.jwt.interfaces.DecodedJWT;
                    import com.auth0.jwt.interfaces.JWTVerifier;
                    import com.auth0.jwt.exceptions.JWTVerificationException;

                    public class VerifyJWT {
                    public static void main(String[] args) {
                    String secret = "your-256-bit-secret"; // Use the actual secret key used for signing
                    Algorithm algorithm = Algorithm.HMAC256(secret);
                    String token = "your-jwt-token"; // Use the actual token to verify

                    try {
                    JWTVerifier verifier = JWT.require(algorithm).build();
                    DecodedJWT jwt = verifier.verify(token);
                    System.out.println("Token is valid.");
                    System.out.println("Claims: " + jwt.getClaims());
                    } catch (JWTVerificationException exception) {
                    System.out.println("Invalid token: " + exception.getMessage());
                    }
                    }
                    }

                    Key Points

                    Payload is not encrypted: The claims can be viewed by anyone who has the token, as they are base64 URL encoded, not encrypted.
                    Tamper detection: If someone alters the payload, the signature verification will fail because the new payload won't match the original signature.
                    Secret Key: The secret key must be kept secure. It is used to sign the token and to verify its integrity. If an attacker gets hold of the secret key, they can generate valid tokens.

                    By following this approach, you ensure that the integrity of the JWT is maintained and any tampering can be detected during the verification process.</p>
                <p>steps for building the architecture of a web app.</p>
                <p>create a PDF report generator with iText free version 2.1.7 or LaTeX</p>
                <p>microservices</p>
                <p>event-driven</p>
                <p>stream with processing (Flink) and state</p>
                <p>security jwt csrf attack</p>
                <p>Encoding vs. Encryption vs. Tokenization

                    Encoding, encryption, and tokenization are three distinct processes that handle data in different ways for various purposes, including data transmission, security, and compliance. In system design, it's important to choose the right approach to handle sensitive information.

                    🔹 Encoding
                    Encoding converts data into a different format using a reversible scheme. For example, Base64 encoding changes binary data into ASCII characters, allowing it to be processed by text-based systems.

                    Encoding is not meant to secure data. Encoded data can be easily decoded using the same scheme without a key.

                    🔹 Encryption
                    Encryption transforms data into a secure format using algorithms and keys. Symmetric encryption uses the same key for both encryption and decryption, while asymmetric encryption uses a public key for encryption and a private key for decryption.

                    Encryption protects data confidentiality by converting readable data (plaintext) into an unreadable format (ciphertext). Only those with the correct key can decrypt and access the original data.

                    🔹 Tokenization
                    Tokenization replaces sensitive data with non-sensitive placeholders called tokens. The mapping between the original data and the token is securely stored in a token vault. Tokens can be used across various systems without revealing the original data, reducing the risk of data breaches.

                    Tokenization is often used to protect credit card information, personal identification numbers, and other sensitive data. It's secure because tokens do not contain any part of the original data and cannot be reverse-engineered. Tokenization is particularly useful for compliance with regulations like PCI DSS.
                </p>
                <p>Ever wonder what it means when your manager asks you to design for "high availability," "high scalability," or "high throughput"? Let me break it down in simple terms.

                    𝗛𝗶𝗴𝗵 𝗔𝘃𝗮𝗶𝗹𝗮𝗯𝗶𝗹𝗶𝘁𝘆 - 𝗞𝗲𝗲𝗽𝗶𝗻𝗴 𝘁𝗵𝗲 𝗦𝗲𝗿𝘃𝗶𝗰𝗲 𝗨𝗽 𝗮𝗻𝗱 𝗥𝘂𝗻𝗻𝗶𝗻𝗴

                    This refers to maximizing uptime of a service, usually targeting 99.9% availability or higher. Each additional nine usually means an exponential increase in complexity. To achieve this, we build in redundancy at multiple levels, with failover systems ready to step in if the main system crashes.

                    𝗛𝗶𝗴𝗵 𝗧𝗵𝗿𝗼𝘂𝗴𝗵𝗽𝘂𝘁 - 𝗛𝗮𝗻𝗱𝗹𝗶𝗻𝗴 𝗛𝗲𝗮𝘃𝘆 𝗟𝗼𝗮𝗱

                    Throughput refers to the number of requests a system can handle per second, measured in transactions per second (TPS) or queries per second (QPS). Common techniques include adding caches, tweaking thread usage, optimizing bottlenecks, and enabling asynchronous processing to handle more simultaneous requests.

                    𝗛𝗶𝗴𝗵 𝗦𝗰𝗮𝗹𝗮𝗯𝗶𝗹𝗶𝘁𝘆 - 𝗚𝗿𝗼𝘄𝗶𝗻𝗴 𝗖𝗮𝗽𝗮𝗰𝗶𝘁𝘆

                    Scalability means a system can expand its workload capacity as needed. To scale out horizontally, it’s common to break services into independent modules or microservices. Leveraging load balancers and service registries enables seamlessly routing requests to new resources.
                </p>
                <p>
                    The Evolution of Message Queues

                    IBM MQ -> RabbitMQ -> Kafka -> Pulsar

                    𝗜𝗕𝗠 𝗠𝗤
                    IBM MQ was launched in 1993. It was renamed to WebSphere MQ in 2002 and to IBM MQ in 2014. IBM MQ is a widely successful product, used extensively in the financial sector.

                    𝗥𝗮𝗯𝗯𝗶𝘁𝗠𝗤
                    RabbitMQ's architecture differs from IBM MQ and more closely resembles Kafka. Producers publish messages to exchanges with specified exchange types like direct, topic, or fanout. Exchanges then route and deliver messages to queues based on predefined bindings and rules. Consumers subscribe to queues and receive messages routed to them by the exchanges.

                    𝗞𝗮𝗳𝗸𝗮
                    In 2011, LinkedIn open-sourced Kafka, a distributed event streaming platform. Kafka optimizes for write throughput and handles real-time data feeds with low latency. Its unified event log enables event streaming. Its simplicity and fault tolerance have allowed it to replace products like AMQP-based queues.

                    𝗣𝘂𝗹𝘀𝗮𝗿
                    Originally developed by Yahoo, Pulsar is an all-in-one messaging and streaming platform. Its cloud-native architecture enables better cluster scaling and partition migration. Pulsar uses a two-layer architecture with a serving layer and persistent layer. It natively supports tiered storage, leveraging cheaper object storage like AWS S3 for long-term persistence.
                </p>
                <p>Managing Data at Scale

                    Modern web-scale applications require specialized databases to handle massive amounts of data. Relational databases alone are often not enough.

                    𝗔𝗻𝗮𝗹𝘆𝘁𝗶𝗰𝗮𝗹 𝗱𝗮𝘁𝗮𝗯𝗮𝘀𝗲𝘀 optimize complex queries across large datasets using column-oriented storage and aggregation. This enables interactive analysis of billions of rows.

                    𝗡𝗼𝗦𝗤𝗟 databases provide flexibility beyond the tabular relational structure:

                    - 𝗞𝗲𝘆-𝘃𝗮𝗹𝘂𝗲 𝘀𝘁𝗼𝗿𝗲𝘀 enable fast, simple data access.
                    - 𝗗𝗼𝗰𝘂𝗺𝗲𝗻𝘁 𝗱𝗮𝘁𝗮𝗯𝗮𝘀𝗲𝘀 store hierarchical JSON data. Highly scalable.
                    - 𝗖𝗼𝗹𝘂𝗺𝗻𝗮𝗿 𝗱𝗮𝘁𝗮𝗯𝗮𝘀𝗲𝘀 optimize analytics via column-based structure.
                    - 𝗚𝗿𝗮𝗽𝗵 databases efficiently store interconnected data.

                    Choosing the right database to match the access patterns and data models is key. A typical web application may use a mix of relational, NoSQL, and other databases to deliver top performance at scale.
                </p>
                <p>Observability: logging, tracing, and metrics.

                    🔹 Logging
                    Logging involves recording discrete events within a system, such as incoming requests or database accesses. It typically generates high volumes of data. The ELK stack (Elasticsearch, Logstash, Kibana) is commonly used to build log analysis platforms. Implementing standardized logging formats across teams for efficient search in log datasets.

                    🔹 Tracing
                    Tracing provides insight into the journey of requests across system components like APIs, load balancers, services, and databases. It is instrumental in identifying performance bottlenecks. OpenTelemetry offers a unified approach for implementing logging, tracing and metrics within a single architecture.

                    🔹 Metrics
                    Metrics represent aggregate data points reflecting a system's operational state, including query rates, API responsiveness, and service latencies. This time-series data is collected in databases like InfluxDB and often processed by tools such as Prometheus, which supports querying and alerting based on specific criteria. Visualization and alerting on metrics can be done in platforms like Grafana, which integrates with various alerting mechanisms like email, SMS, or Slack.
                </p>
                <p>Securing REST APIs

                    It is important to make sure that only approved users and applications can access or make changes to resources in our API.

                    Here are some common was to secure REST APIs:

                    1. 𝗕𝗮𝘀𝗶𝗰 𝗔𝘂𝘁𝗵𝗲𝗻𝘁𝗶𝗰𝗮𝘁𝗶𝗼𝗻

                    This sends a username and password with each request to the API. It’s straightforward, but not very secure unless used witn encryption like HTTPS.

                    Good for simpler apps where advanced security is not critical. Should be combined with encrypted connections.

                    2. 𝗧𝗼𝗸𝗲𝗻 𝗔𝘂𝘁𝗵𝗲𝗻𝘁𝗶𝗰𝗮𝘁𝗶𝗼𝗻

                    This uses tokens, like JSON Web Tokens (JWT), that are exchanged between the client app and server. Login information is not sent with each request.

                    Better for more secure and scalable apps where not sending credentials each time is essential.

                    3. 𝗢𝗽𝗲𝗻𝗜𝗗 𝗖𝗼𝗻𝗻𝗲𝗰𝘁 𝗮𝗻𝗱 𝗢𝗔𝘂𝘁𝗵

                    These allow limited third-party access to user data without exposing passwords. OpenID Connect handles user authentication and OAuth handles authorization.

                    Perfect when third-party services need controlled access to user data, like when integrating with Google, Facebook, or Twitter.

                    4. 𝗔𝗣𝗜 𝗞𝗲𝘆 𝗔𝘂𝘁𝗵𝗲𝗻𝘁𝗶𝗰𝗮𝘁𝗶𝗼𝗻

                    This gives unique keys to users or apps which are sent in request headers or query parameters. Simple to implement but may not be as robust as token or OAuth methods.

                    Good for basic access control when security needs are moderate. Allows access to specific API functionalities without complex user permissions.
                    +++++++ from comment
                    Here are some additional methods for securing REST APIs that you might consider:

                    5. Mutual SSL/TLS Certificates:
                    Instead of simply encrypting data with HTTPS, mutual SSL/TLS certificates require authentication from both sides. This means that the server and the client authenticate each other before establishing a connection, thus enhancing security.

                    6. Role-Based Access Control (RBAC)
                    This method involves defining specific roles for users and granting permissions based on these roles. Thus, only users with appropriate permissions can access certain parts of the API.

                    7. Rate and Frequency Limiting of Requests
                    By limiting the number of requests a user can make within a given period, you can reduce the risks of API abuse or overload. This can be achieved using quotas or rate limits.

                    8. Audit and Logging of Activities
                    Implementing an activity logging system allows you to track actions performed on the API, making it easier to detect and respond to potential unauthorized access attempts or suspicious behavior.

                    By choosing from these methods (or combining them), you can enhance the security of your REST API according to your specific needs and the level of protection required for the data being handled.
                </p>
                <p>Data is cached everywhere, from the client-facing side to backend systems

                    Let's look at the many caching layers:

                    1. Client Apps: Browsers cache HTTP responses. Server responses include caching directives in headers. Upon subsequent requests, browsers may serve cached data if still fresh.

                    2. Content Delivery Networks: CDNs cache static content like images, stylesheets, and JavaScript files. They serve cached content from locations closer to users, reducing latency and load times.

                    3. Load Balancers: Some load balancers cache frequently requested data. This allows serving responses without engaging backend servers, reducing load and response times.

                    4. Message Brokers: Systems like Kafka can cache messages on disk per a retention policy. Consumers then pull messages according to their own schedule.

                    5. Services: Individual services often employ caching to improve data retrieval speeds, first checking in-memory caches before querying databases. Services may also utilize disk caching for larger datasets.

                    6. Distributed Caches: Systems like Redis cache key-value pairs across services, providing faster read/write capabilities compared to traditional databases.

                    7. Full-text Search Engines: Platforms like Elasticsearch index data for efficient text search. This index is effectively a form of cache, optimized for quick text search retrieval.

                    8. Databases: There are specialized mechanisms to enhance performance, some of which include caching concepts:
                    - Bufferpool: This is a cache within the database that holds copies of data pages. It allows for quick reads and writes to temporary storage in memory, reducing the need to access data from disk.
                    - Materialized Views: They are similar to caches in that they store the results of computationally expensive queries. The database can return these precomputed results quickly, rather than recalculating them.
                </p>
                <pre>// src/main/java/com/example/filters/CORSFilter.java
                    package com.example.filters;

                    import javax.servlet.Filter;
                    import javax.servlet.FilterChain;
                    import javax.servlet.FilterConfig;
                    import javax.servlet.ServletException;
                    import javax.servlet.ServletRequest;
                    import javax.servlet.ServletResponse;
                    import javax.servlet.http.HttpServletResponse;
                    import java.io.IOException;

                    public class CORSFilter implements Filter {

                    @Override
                    public void init(FilterConfig filterConfig) throws ServletException {
                    }

                    @Override
                    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)
                    throws IOException, ServletException {
                    HttpServletResponse res = (HttpServletResponse) response;
                    res.setHeader("Access-Control-Allow-Origin", "http://localhost:3000"); // React app's URL
                    res.setHeader("Access-Control-Allow-Methods", "GET, POST, DELETE, PUT");
                    res.setHeader("Access-Control-Allow-Headers", "Content-Type");
                    res.setHeader("Access-Control-Allow-Credentials", "true");
                    chain.doFilter(request, response);
                    }

                    @Override
                    public void destroy() {
                    }
                    }
                </pre>
            </div>
        </div>
    </div>
</main>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
</body>
</html>
